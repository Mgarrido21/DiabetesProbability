# -*- coding: utf-8 -*-
"""A01208955_HandIn.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/178pHfliLUyLiakWo-bPVsdKeXMVW2Aph

Hand In - Montserrat Garrido | A01208955

This programm exists to help people determine their likelihood of diabetes based on family history. 

The Data Set is originally from the National Institute of Diabetes and Digestive and Kidney Diseases. Obtained from https://www.kaggle.com/datasets/mathchi/diabetes-data-set?resource=download

##Data Setup
"""

#Mount content
from google.colab import drive
drive.mount('/content/drive')

# Commented out IPython magic to ensure Python compatibility.
#find files
# %cd "/content/drive/MyDrive/ONCEAVO/INTELLIGENT SYS/Hand In"
!pwd
!ls

#Libraries
import pandas as pd
import numpy as np
from pylab import rcParams
import seaborn as sns
import matplotlib.pyplot as plt #used to visualize stuff
from sklearn.datasets import make_blobs
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score, mean_squared_error, r2_score, adjusted_rand_score
from sklearn.cluster import DBSCAN
from sklearn.datasets import make_moons
from sklearn.tree import DecisionTreeClassifier
from sklearn.preprocessing import LabelEncoder, MinMaxScaler, StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
import math
from sklearn import linear_model
from sklearn import tree 
import graphviz
from sklearn.decomposition import PCA

"""# Decision Tree"""

columns = ["Glucose", "BloodPressure", "Insulin", "BMI", "DiabetesPF", "Age", "Outcome"]
df = pd.read_csv('diabetes.csv',names = columns, skiprows=1)
df=df.drop(['DiabetesPF'], axis=1)
df.head()

df_x = df[["Glucose", "BloodPressure", "Insulin", "BMI", "Age"]]
df_y = df[["Outcome"]]

#Generate Train and Test Sets
x_train, x_test, y_train, y_test = train_test_split(df_x, df_y, test_size = 0.2, random_state = 0)
clf = tree.DecisionTreeClassifier(random_state=0, max_depth=4)
clf = clf.fit(x_train, y_train)
#tree_clf.fit(x_train, y_train)

y_pred = clf.predict(x_test)

accuracy_score(y_test, y_pred)

dot = tree.export_graphviz(clf, out_file=None, 
                           feature_names=["Glucose", "BloodPressure", "Insulin", "BMI", "Age"][:],
                           class_names=None,
                           filled=True, rounded=True,  
                           special_characters=True) 
graph = graphviz.Source(dot) 
graph

#Initialize
print("Hello, we are here to determine whether you should keep eating pan de muerto or consider changing to salads")
print("Please enter the requested information below")
Glucose = float(input("Glucose Levels: "))
BloodPressure = float(input("Diastolic Blood Pressure (mm Hg): "))
Insulin = float(input("Insulin Levels (mu U/ml): "))
BMI = int(input("Body Mass Index: "))
Age = int(input("Age: "))

numpy_array = np.array([[Glucose, BloodPressure, Insulin, BMI, Age]])
new_df = pd.DataFrame(numpy_array, columns=['Glucose','BloodPressure','Insulin','BMI', 'Age'])

prediction = clf.predict(new_df)
if(prediction[0]==0):
    print('Your numbers seem fine, you may add Nutella to your Pan de Muerto')
else:
     print('You should probably stop eating Pan de Muerto and see a doctor')

"""# KMEANS
I am going to use the clustering method of KMeans because I think it suits better the data distribution presented on the graph we generated
"""

#I am reading the file again in order to exclude the 'Outcome' column
df = pd.read_csv('diabetes.csv',names = columns, skiprows=1, usecols=range(6))
df.head()

#PCA
pca = PCA(n_components=5)
pca.fit(df)
df2 = pca.transform(df)

df2 = pd.DataFrame(df2)
df2.index = df.index
df2.columns = ['PCA1','PCA2','PCA3','PCA4','PCA5']
df2.head()

#Scale Data
scaler = StandardScaler()
scaled_features = scaler.fit_transform(df2)

df_x = df[["Glucose", "BloodPressure", "Insulin", "BMI", "Age"]]
df_y = df[["DiabetesPF"]]

df2.plot(
        kind='scatter',
        x='PCA3',y='PCA4',
        figsize=(16,8))

kmeans = KMeans(n_clusters=4)
clusters = kmeans.fit(df2)
df['cluster'] = pd.Series(clusters.labels_, index=df.index)

kmeans = KMeans(
    n_clusters = 4,
    init="k-means++",
    n_init=10,
    max_iter=500,
    random_state=42 )

kmeans.fit(df2)
df_t = scaled_features.T
plt.scatter(df_t[0], df_t[1], c=kmeans.labels_)

kmeans.inertia_

# Find the locations of the centroids
kmeans.cluster_centers_

# Find the number of iterations required to converge
kmeans.n_iter_

#A score close to 0.0 indicates random assignments, and a score close to 1 indicates perfectly labeled clusters.
#ari_kmeans = adjusted_rand_score(true_labels, kmeans.labels_)
#ari_dbscan = adjusted_rand_score(true_labels, dbscan.labels_)

NewCluster = kmeans.labels_[3]
print('%f', NewCluster)

print("\nYou are in group: ", NewCluster)
requeridas = 0
if(NewCluster == 0):
    print("This group represents that your numbers came out almost perfectly, yo may keep eating Pan de Muerto and even add an Abuelita hot chocolate")

if(NewCluster == 1):
    print("This group represents that your numbers came out so-so, you could keep eating Pan de Muerto but maybe exercise a bit")

if(NewCluster == 2):
    print("This group represents that your numbers came out bad, but not terrible, you should consider limiting your Pan de Muerto intake.")

if(NewCluster == 3):
    print("This group represents that your numbers came out terribly, you should consider trying lettuce and visiting a doctor")
